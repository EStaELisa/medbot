from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification

# Beispiel-Daten
data = [
    {"query": "What are the symptoms of sepsis?", "entities": [("sepsis", "DIAGNOSE")]},
    {"query": "What disease could nausea suggest?", "entities": [("nausea", "SYMPTOM")]},
    {"query": "The patient has fever and chills.", "entities": [("fever", "SYMPTOM"), ("chills", "SYMPTOM")]},
    {"query": "They might have pneumonia.", "entities": [("pneumonia", "DIAGNOSE")]}
]

# Lade den Tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Daten vorbereiten


def prepare_data(data):
    tokenized_data = []
    for entry in data:
        text = entry["query"]
        tokens = tokenizer.tokenize(text)
        labels = ["O"] * len(tokens)

        # Entit√§ten markieren
        for entity, label in entry["entities"]:
            entity_tokens = tokenizer.tokenize(entity)
            for i in range(len(tokens) - len(entity_tokens) + 1):
                if tokens[i:i+len(entity_tokens)] == entity_tokens:
                    labels[i] = f"B-{label}"
                    for j in range(1, len(entity_tokens)):
                        labels[i+j] = f"I-{label}"

        tokenized_data.append({"tokens": tokens, "labels": labels})
    return tokenized_data


prepared_data = prepare_data(data)

# Konvertiere die Daten in ein Hugging Face Dataset


def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(
        examples["tokens"],
        truncation=True,
        is_split_into_words=True,
        padding="max_length",
        max_length=128
    )
    labels = []
    for i, label in enumerate(examples["labels"]):
        label_ids = []
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_id = None
        for word_id in word_ids:
            if word_id is None:
                label_ids.append(-100)
            elif word_id != previous_word_id:
                label_ids.append(label[word_id])
            else:
                label_ids.append(label[word_id] if label[word_id].startswith("I-") else -100)
            previous_word_id = word_id
        labels.append(label_ids)
    tokenized_inputs["labels"] = labels
    return tokenized_inputs


# Konvertiere die Labels in Indizes
label_to_id = {"O": 0, "B-SYMPTOM": 1, "I-SYMPTOM": 2, "B-DIAGNOSE": 3, "I-DIAGNOSE": 4}
id_to_label = {v: k for k, v in label_to_id.items()}

# Label in numerisches Format umwandeln
for item in prepared_data:
    item["labels"] = [label_to_id[label] for label in item["labels"]]

dataset = Dataset.from_list(prepared_data)
dataset = dataset.map(tokenize_and_align_labels, batched=True)

# Lade das vortrainierte Modell
model = AutoModelForTokenClassification.from_pretrained("bert-base-uncased", num_labels=len(label_to_id))

# Datenkollator
data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)

# Trainingsargumente definieren
training_args = TrainingArguments(
    output_dir="./ner_model",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01
)

# Trainer initialisieren
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    eval_dataset=dataset,
    tokenizer=tokenizer,
    data_collator=data_collator
)

# Modell trainieren
trainer.train()
